{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Zip-YY8wmlg0"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical\n",
        "import random\n",
        "from Fire_Fighter_Env import FireFighterEnv, CellType\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "AppdMZW5sVvL"
      },
      "outputs": [],
      "source": [
        "# Policy Network Definition\n",
        "class PolicyNetwork(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(PolicyNetwork, self).__init__()\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Linear(input_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, output_size),\n",
        "            nn.Softmax(dim=-1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "7IY3GGuUsY87"
      },
      "outputs": [],
      "source": [
        "# Policy Gradient Agent Class\n",
        "class PolicyGradientAgent:\n",
        "    def __init__(self, env, learning_rate=0.001, gamma=0.95, hidden_size=512):\n",
        "        self.env = env\n",
        "        input_size = 6  # agent_pos (2), nearest_fire_pos (2), steps_left (1), score (1)\n",
        "        self.policy = PolicyNetwork(input_size, hidden_size, 4)\n",
        "        self.optimizer = optim.Adam(self.policy.parameters(), lr=learning_rate)\n",
        "        self.gamma = gamma\n",
        "        self.log_probs = []\n",
        "        self.rewards = []\n",
        "        self.eps = np.finfo(np.float32).eps.item()\n",
        "\n",
        "    def state_to_tensor(self, state):\n",
        "        fire_locs = np.argwhere(state['grid'] == CellType.FIRE.value)\n",
        "        if len(fire_locs) > 0:\n",
        "            agent_x, agent_y = state['agent_pos']\n",
        "            distances = np.sqrt(((fire_locs - [agent_x, agent_y])**2).sum(axis=1))\n",
        "            nearest_idx = np.argmin(distances)\n",
        "            nearest_fire_pos = fire_locs[nearest_idx]\n",
        "        else:\n",
        "            nearest_fire_pos = state['agent_pos']\n",
        "\n",
        "        agent_pos = torch.FloatTensor(state['agent_pos']) / self.env.size[0]\n",
        "        fire_pos = torch.FloatTensor(nearest_fire_pos) / self.env.size[0]\n",
        "        steps_left = torch.FloatTensor([state['steps_left'] / self.env.max_steps])\n",
        "        score = torch.FloatTensor([state['score'] / 1000.0])\n",
        "\n",
        "        return torch.cat([agent_pos, fire_pos, steps_left, score]).unsqueeze(0)\n",
        "\n",
        "    def select_action(self, state):\n",
        "        state_tensor = self.state_to_tensor(state)\n",
        "        probs = self.policy(state_tensor)\n",
        "        m = Categorical(probs)\n",
        "        action = m.sample()\n",
        "        self.log_probs.append(m.log_prob(action))\n",
        "        return action.item(), m.entropy()\n",
        "\n",
        "    def update_policy(self, entropies, episode):\n",
        "        discounted_rewards = []\n",
        "        running_reward = 0\n",
        "        for r in self.rewards[::-1]:\n",
        "            running_reward = r + self.gamma * running_reward\n",
        "            discounted_rewards.insert(0, running_reward)\n",
        "\n",
        "        discounted_rewards = torch.FloatTensor(discounted_rewards)\n",
        "        discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / \\\n",
        "                            (discounted_rewards.std() + self.eps)\n",
        "\n",
        "        entropy_coeff = 0.1 * (1 - episode / 2000)\n",
        "        policy_loss = []\n",
        "        for log_prob, reward, entropy in zip(self.log_probs, discounted_rewards, entropies):\n",
        "            policy_loss.append(-log_prob * reward - entropy_coeff * entropy)\n",
        "\n",
        "        policy_loss_tensor = torch.stack(policy_loss).sum()  # Total loss for the episode\n",
        "        avg_loss = policy_loss_tensor.item() / len(self.log_probs)  # Average loss per step\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        policy_loss_tensor.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(self.policy.parameters(), 1.0)\n",
        "        self.optimizer.step()\n",
        "\n",
        "        self.log_probs = []\n",
        "        self.rewards = []\n",
        "        return avg_loss  # Return average loss for reporting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "OKgPWjNNuVwU"
      },
      "outputs": [],
      "source": [
        "def train_policy_gradient(env, num_episodes=500, max_steps=200):\n",
        "    agent = PolicyGradientAgent(env)\n",
        "    best_reward = float('-inf')\n",
        "    rewards_history = []\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        total_reward = 0\n",
        "        entropies = []\n",
        "\n",
        "        for t in range(max_steps):\n",
        "            action, entropy = agent.select_action(state)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            fire_locs = np.argwhere(state['grid'] == CellType.FIRE.value)\n",
        "            if len(fire_locs) > 0:\n",
        "                old_dist = min(np.sqrt(((fire_locs - state['agent_pos'])**2).sum(axis=1)))\n",
        "                new_dist = min(np.sqrt(((fire_locs - next_state['agent_pos'])**2).sum(axis=1)))\n",
        "                if new_dist < old_dist:\n",
        "                    reward += 20\n",
        "\n",
        "            agent.rewards.append(reward)\n",
        "            entropies.append(entropy)\n",
        "            total_reward += reward\n",
        "            state = next_state\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        avg_loss = agent.update_policy(entropies, episode)  # Get average loss\n",
        "        rewards_history.append(total_reward)\n",
        "        print(f\"Episode {episode + 1}, Total Reward: {total_reward:.2f}, Avg Loss: {avg_loss:.4f}\")\n",
        "\n",
        "        if total_reward > best_reward:\n",
        "            best_reward = total_reward\n",
        "            torch.save(agent.policy.state_dict(), 'best_policy.pt')\n",
        "            print(f\"New best reward: {best_reward:.2f} - Model saved\")\n",
        "\n",
        "    return agent, rewards_history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BocZl0RetqiC"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
