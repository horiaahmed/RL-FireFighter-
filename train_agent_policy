import tensorflow as tf
import numpy as np
import random
import tkinter as tk
from Fire_Fighter_Env import FireFighterEnv, FireGameUI  
from Fire_Fighter_Env import FireFighterEnv

# Load the policy model and the trained weights
model = tf.keras.models.load_model("Best_PG_Model.keras")

# Initialize the environment
env = FireFighterEnv(size=9, fire_spawn_delay=10, max_steps=200)

# Initialize the UI
root = tk.Tk()
ui = FireGameUI(root, env)

# Initialize state and done flag
state = env.reset()
state = state['grid']
done = False

# Define hyperparameters for Policy Gradient
epsilon = 0.5  # Exploration factor (for action sampling)
gamma = 0.99  # Discount factor
rewards_history = []  # Store rewards for backpropagation
states_history = []  # Store states for backpropagation
actions_history = []  # Store actions for backpropagation

def run_episode():
    global state, done, rewards_history, states_history, actions_history
    if not done:
        # Sample an action from the policy (based on the softmax output of the model)
        action_probs = model.predict(np.expand_dims(state, axis=0), verbose=0)  # Output is probabilities for each action
        action = np.random.choice(len(action_probs[0]), p=action_probs[0])  # Sample action based on probabilities

        # Take a step in the environment
        next_state, reward, done, _ = env.step(action)
        next_state = next_state['grid']

        # Store the rewards, actions, and states for policy gradient updates
        rewards_history.append(reward)
        actions_history.append(action)
        states_history.append(state)

        state = next_state
        ui.render(action)

    # If the episode is done, update the policy using the collected rewards and states
    if done:
        # Compute returns (discounted sum of future rewards)
        returns = []
        discounted_reward = 0
        for reward in reversed(rewards_history):
            discounted_reward = reward + gamma * discounted_reward
            returns.insert(0, discounted_reward)

        # Normalize the returns for stability
        returns = np.array(returns)
        returns = (returns - np.mean(returns)) / (np.std(returns) + 1e-7)

        # Perform a policy update (typically using a loss function like categorical cross-entropy)
        actions_history = np.array(actions_history)
        states_history = np.array(states_history)
        returns = np.array(returns)

        # Train the model (policy gradient step)
        with tf.GradientTape() as tape:
            # Compute probabilities for the taken actions
            action_probs = model(states_history)
            action_probs = tf.gather(action_probs, actions_history, axis=1, batch_dims=1)
            # Compute the loss (negative log likelihood of the chosen actions weighted by the returns)
            loss = -tf.reduce_mean(tf.math.log(action_probs) * returns)

        # Calculate gradients and apply them to the model
        grads = tape.gradient(loss, model.trainable_variables)
        optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)
        optimizer.apply_gradients(zip(grads, model.trainable_variables))

        # Reset the histories after the update
        rewards_history = []
        states_history = []
        actions_history = []

        print(f"Episode completed with total reward: {sum(rewards_history)}")

    # Schedule the next step of the game
    root.after(1000, run_episode)

# Start the first episode
root.after(0, run_episode)
root.mainloop()
